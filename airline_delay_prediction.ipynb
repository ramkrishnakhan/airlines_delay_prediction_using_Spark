#%%
from itertools import groupby

# Cell 1: Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from fontTools.ttLib.tables.S__i_l_f import assemble
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier


#%%
from pyspark.sql import SparkSession
#%%
spark = (
    SparkSession.builder
    .appName("FlightDelayRF")
    .master("local[*]")
    .config("spark.driver.memory", "6g")   # or 8g if you have it
    .config("spark.executor.memory", "6g")
    .getOrCreate()
)

#%%
spark
#%%
df = spark.read.csv("/Users/ramkrishnakhan/PycharmProjects/airlines_delay_prediction(Spark)/flight_data_2024.csv",header=True,inferSchema=True)
df.printSchema()

#%%
df.show(1)
#%%
print(df.count())
#%%
df.describe().show()
#%%
###checking for missing values
from pyspark.sql.functions import col, sum as spark_sum
missing_count=df.select([
    spark_sum(col(c).isNull().cast("int")).alias(c) for c in df.columns
])
missing_count.show()
#%%
###dropping rows with missing values in critical columns
df=df.na.drop(subset=['dep_time','dep_delay','arr_time','arr_delay',])
#%%
cols_to_exclude = [
    "dep_time",
    "dep_delay",
    "taxi_out",
    "wheels_off",
    "wheels_on",
    "taxi_in",
    "arr_time",
    "actual_elapsed_time",
    "air_time",
    "carrier_delay",
    "weather_delay",
    "nas_delay",
    "security_delay",
    "late_aircraft_delay",
    "cancellation_code",
    "cancelled",
    "diverted",
    "fl_date",
    "op_carrier_fl_num",
    "origin_city_name",
    "dest_city_name"
]

#%%
###drooping unnecessary/post-departure columns as we are trying to predict delay before the flight takes off
df=(df
    .drop(*cols_to_exclude)
    .withColumn("label", (df.arr_delay > 15).cast("int"))
)
#%%
from pyspark.sql.functions import col,floor
df=df.withColumn(
    "crs_dep_hour",
   floor(col("crs_dep_time") / 100).cast("int"))
#%%
df=df.drop("arr_delay","crs_dep_time")
#%%
df.printSchema()
#%%
df.select('label').distinct().show()
#%%

#%%

#%%
cat_cols = [
    "op_unique_carrier",
    "origin",
    "dest",
    "origin_state_nm",
    "dest_state_nm"
]

#%%
num_cols = [
    "year",
    "month",
    "day_of_month",
    "day_of_week",
    "crs_dep_hour",
    "crs_elapsed_time",
    "distance",
    "carrier_delay_rate",
    "origin_delay_rate",
    "dest_delay_rate",
    "route_delay_rate"
]


#%%
from pyspark.ml.feature import StringIndexer
indexers=[
    StringIndexer(
        inputCol=c,
        outputCol=f"{c}_idx",
        handleInvalid="keep"
    )
    for c in cat_cols
]
#%%
from pyspark.ml.feature import VectorAssembler

feature_cols=[f"{c}_idx" for c in cat_cols]+num_cols

assemble=VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)
#%%
from pyspark.ml.classification import RandomForestClassifier

rf=RandomForestClassifier(
    labelCol="label",
    featuresCol="features",
    numTrees=30,
    maxDepth=8,
    maxBins=347,
    minInstancesPerNode=100,
    seed=42
)
#%%
from pyspark.ml import Pipeline
pipeline=Pipeline(
    stages=indexers+[assemble,rf]
)
#%%
train_df = df.filter("month <= 9")
test_df  = df.filter("month > 9")

#%%
train_df.count()

#%%
###Adding historical features
#%%
from pyspark.sql.functions import avg
###Carrier delay rate
carries_hist=(
    train_df
    .groupby("op_unique_carrier")
    .agg(avg("label").alias("carrier_delay_rate"))
)
#%%
###Origin airport delay rate
origin_hist=(
    train_df
    .groupby("origin")
    .agg(avg("label").alias("origin_delay_rate"))
)
#%%
####Destination airport delay rate
dest_hist=(
    train_df
    .groupby("dest")
    .agg(avg("label").alias("dest_delay_rate"))
)
#%%
###route delay rate
route_hist = (
    train_df
    .groupBy("origin", "dest")
    .agg(avg("label").alias("route_delay_rate"))
)

#%%
def add_hist_features(df):
    return (
        df
        .join(carries_hist, "op_unique_carrier", "left")
        .join(origin_hist, "origin", "left")
        .join(dest_hist, "dest", "left")
        .join(route_hist, ["origin", "dest"], "left")
        .fillna(0.0)
    )

train_df = add_hist_features(train_df)
test_df  = add_hist_features(test_df)

#%%

#%%
rf_model = pipeline.fit(train_df)

#%%
predictions = rf_model.transform(test_df)

predictions.select(
    "label", "prediction", "probability"
).show(5, truncate=False)

#%%
from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator=BinaryClassificationEvaluator(
    labelCol="label",
    metricName="areaUnderROC"
)
auc=evaluator.evaluate(predictions)
print(f"Test AUC: {auc:.4f}")
#%%
predictions.groupBy("label").count().show()

#%%
rf_stage = rf_model.stages[-1]

for name, score in sorted(
    zip(feature_cols, rf_stage.featureImportances),
    key=lambda x: x[1],
    reverse=True
):
    print(f"{name:25s} {score:.4f}")

#%%

#%%
from pyspark.ml.classification import GBTClassifier

gbt = GBTClassifier(
    labelCol="label",
    featuresCol="features",
    maxDepth=6,
    maxIter=40,
    maxBins=347,
    seed=42
)

#%%
from pyspark.ml import Pipeline

pipeline = Pipeline(
    stages=indexers + [assemble, gbt]
)

#%%
train_df = train_df.cache()
train_df.count()   # materialize

model = pipeline.fit(train_df)

#%%
preds = model.transform(test_df)

from pyspark.ml.evaluation import BinaryClassificationEvaluator

evaluator = BinaryClassificationEvaluator(
    labelCol="label",
    metricName="areaUnderROC"
)

auc = evaluator.evaluate(preds)
print("AUC:", auc)

#%%
model_path = "/Users/ramkrishnakhan/PycharmProjects/airlines_delay_prediction(Spark)/"

model.write().overwrite().save(model_path)
