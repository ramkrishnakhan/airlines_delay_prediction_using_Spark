{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:01:15.509155Z",
     "start_time": "2025-12-13T17:01:14.953173Z"
    }
   },
   "source": [
    "from itertools import groupby\n",
    "\n",
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from fontTools.ttLib.tables.S__i_l_f import assemble\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:01:21.524702Z",
     "start_time": "2025-12-13T17:01:21.464337Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql import SparkSession",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:01:31.101065Z",
     "start_time": "2025-12-13T17:01:23.810595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"FlightDelayRF\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")   # or 8g if you have it\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/13 22:31:26 WARN Utils: Your hostname, RAMKRISHNAs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.68.100 instead (on interface en0)\n",
      "25/12/13 22:31:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/13 22:31:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:01:34.717839Z",
     "start_time": "2025-12-13T17:01:34.430702Z"
    }
   },
   "cell_type": "code",
   "source": "spark",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1150616a0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FlightDelayRF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:01:54.817010Z",
     "start_time": "2025-12-13T17:01:41.089340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.csv(\"/Users/ramkrishnakhan/PycharmProjects/airlines_delay_prediction(Spark)/flight_data_2024.csv\",header=True,inferSchema=True)\n",
    "df.printSchema()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- fl_date: date (nullable = true)\n",
      " |-- op_unique_carrier: string (nullable = true)\n",
      " |-- op_carrier_fl_num: double (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- origin_city_name: string (nullable = true)\n",
      " |-- origin_state_nm: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- dest_city_name: string (nullable = true)\n",
      " |-- dest_state_nm: string (nullable = true)\n",
      " |-- crs_dep_time: integer (nullable = true)\n",
      " |-- dep_time: double (nullable = true)\n",
      " |-- dep_delay: double (nullable = true)\n",
      " |-- taxi_out: double (nullable = true)\n",
      " |-- wheels_off: double (nullable = true)\n",
      " |-- wheels_on: double (nullable = true)\n",
      " |-- taxi_in: double (nullable = true)\n",
      " |-- crs_arr_time: integer (nullable = true)\n",
      " |-- arr_time: double (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- cancelled: integer (nullable = true)\n",
      " |-- cancellation_code: string (nullable = true)\n",
      " |-- diverted: integer (nullable = true)\n",
      " |-- crs_elapsed_time: double (nullable = true)\n",
      " |-- actual_elapsed_time: double (nullable = true)\n",
      " |-- air_time: double (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- carrier_delay: integer (nullable = true)\n",
      " |-- weather_delay: integer (nullable = true)\n",
      " |-- nas_delay: integer (nullable = true)\n",
      " |-- security_delay: integer (nullable = true)\n",
      " |-- late_aircraft_delay: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T16:10:21.513240Z",
     "start_time": "2025-12-13T16:10:21.381035Z"
    }
   },
   "cell_type": "code",
   "source": "df.show(1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-----------+----------+-----------------+-----------------+------+----------------+---------------+----+--------------+-------------+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "|year|month|day_of_month|day_of_week|   fl_date|op_unique_carrier|op_carrier_fl_num|origin|origin_city_name|origin_state_nm|dest|dest_city_name|dest_state_nm|crs_dep_time|dep_time|dep_delay|taxi_out|wheels_off|wheels_on|taxi_in|crs_arr_time|arr_time|arr_delay|cancelled|cancellation_code|diverted|crs_elapsed_time|actual_elapsed_time|air_time|distance|carrier_delay|weather_delay|nas_delay|security_delay|late_aircraft_delay|\n",
      "+----+-----+------------+-----------+----------+-----------------+-----------------+------+----------------+---------------+----+--------------+-------------+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "|2024|    1|           1|          1|2024-01-01|               9E|           4814.0|   JFK|    New York, NY|       New York| DTW|   Detroit, MI|     Michigan|        1252|  1247.0|     -5.0|    31.0|    1318.0|   1442.0|    7.0|        1508|  1449.0|    -19.0|        0|             NULL|       0|           136.0|              122.0|    84.0|   509.0|            0|            0|        0|             0|                  0|\n",
      "+----+-----+------------+-----------+----------+-----------------+-----------------+------+----------------+---------------+----+--------------+-------------+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T16:10:26.189721Z",
     "start_time": "2025-12-13T16:10:24.418651Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.count())",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7079081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T16:11:27.025837Z",
     "start_time": "2025-12-13T16:10:28.502731Z"
    }
   },
   "cell_type": "code",
   "source": "df.describe().show()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+------------------+------------------+-----------------+------------------+-------+----------------+---------------+-------+--------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+------------------+------------------+--------------------+-----------------+--------------------+-----------------+-------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
      "|summary|   year|            month|      day_of_month|       day_of_week|op_unique_carrier| op_carrier_fl_num| origin|origin_city_name|origin_state_nm|   dest|dest_city_name|dest_state_nm|      crs_dep_time|          dep_time|         dep_delay|          taxi_out|        wheels_off|         wheels_on|         taxi_in|      crs_arr_time|          arr_time|         arr_delay|           cancelled|cancellation_code|            diverted| crs_elapsed_time|actual_elapsed_time|          air_time|         distance|     carrier_delay|     weather_delay|         nas_delay|      security_delay|late_aircraft_delay|\n",
      "+-------+-------+-----------------+------------------+------------------+-----------------+------------------+-------+----------------+---------------+-------+--------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+------------------+------------------+--------------------+-----------------+--------------------+-----------------+-------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
      "|  count|7079081|          7079081|           7079081|           7079081|          7079081|           7079080|7079081|         7079081|        7079081|7079081|       7079081|      7079081|           7079081|           6986422|           6986111|           6983347|           6983347|           6981225|         6981225|           7079081|           6981227|           6965267|             7079081|            96315|             7079081|          7079080|            6965267|           6965267|          7079081|           7079081|           7079081|           7079081|             7079081|            7079081|\n",
      "|   mean| 2024.0|6.584868290107148|15.784451258574382| 3.981944972800848|             NULL|2504.6225008051892|   NULL|            NULL|           NULL|   NULL|          NULL|         NULL|1327.2998243698582|1330.7651906798644| 12.67708199884027|17.904450115395957|1353.5419099179805|1458.7237805685966|8.32603017378755|1491.3648473580115|1461.8542438170252|  7.09824533646736|0.013605579594300447|             NULL|0.002471931031725728|146.7664685241585| 141.21513992213076|114.99971558879221|833.9061503887299|5.0604400486447325|0.8752369128139655|2.7718278686174096|0.025416858487704832|  5.929301698906962|\n",
      "| stddev|    0.0|3.396806482581382| 8.786432524408605|2.0122793474688465|             NULL|1652.2525050789206|   NULL|            NULL|           NULL|   NULL|          NULL|         NULL|493.03062067684596|509.47790732596815|56.059970271063335| 9.677409542977358| 512.0474556121318| 540.4327503490271|6.84451028490913| 519.4384847499432| 545.9961718873008|57.991270554502776| 0.11584675089960998|             NULL| 0.04965703310735693|72.38692355437207|  72.30781095537496| 70.36955312409242|596.2535939770951| 35.72703064643097|15.429742946024973|15.464847064916658|   1.422602388222699| 31.061352608145757|\n",
      "|    min|   2024|                1|                 1|                 1|               9E|               1.0|    ABE|    Aberdeen, SD|        Alabama|    ABE|  Aberdeen, SD|      Alabama|                 1|               1.0|             -96.0|               1.0|               1.0|               1.0|             1.0|                 1|               1.0|            -126.0|                   0|                A|                   0|           -160.0|               15.0|               5.0|             11.0|                 0|                 0|                 0|                   0|                  0|\n",
      "|    max|   2024|               12|                31|                 7|               YX|            8819.0|    YUM|        Yuma, AZ|        Wyoming|    YUM|      Yuma, AZ|      Wyoming|              2400|            2400.0|            3777.0|             214.0|            2400.0|            2400.0|           444.0|              2359|            2400.0|            3803.0|                   1|                D|                   1|           1326.0|              792.0|             723.0|           5095.0|              3689|              1804|              2700|                1164|               2690|\n",
      "+-------+-------+-----------------+------------------+------------------+-----------------+------------------+-------+----------------+---------------+-------+--------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+------------------+------------------+--------------------+-----------------+--------------------+-----------------+-------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T16:11:35.764578Z",
     "start_time": "2025-12-13T16:11:27.032502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###checking for missing values\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "missing_count=df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "])\n",
    "missing_count.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-----------+-------+-----------------+-----------------+------+----------------+---------------+----+--------------+-------------+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "|year|month|day_of_month|day_of_week|fl_date|op_unique_carrier|op_carrier_fl_num|origin|origin_city_name|origin_state_nm|dest|dest_city_name|dest_state_nm|crs_dep_time|dep_time|dep_delay|taxi_out|wheels_off|wheels_on|taxi_in|crs_arr_time|arr_time|arr_delay|cancelled|cancellation_code|diverted|crs_elapsed_time|actual_elapsed_time|air_time|distance|carrier_delay|weather_delay|nas_delay|security_delay|late_aircraft_delay|\n",
      "+----+-----+------------+-----------+-------+-----------------+-----------------+------+----------------+---------------+----+--------------+-------------+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "|   0|    0|           0|          0|      0|                0|                1|     0|               0|              0|   0|             0|            0|           0|   92659|    92970|   95734|     95734|    97856|  97856|           0|   97854|   113814|        0|          6982766|       0|               1|             113814|  113814|       0|            0|            0|        0|             0|                  0|\n",
      "+----+-----+------------+-----------+-------+-----------------+-----------------+------+----------------+---------------+----+--------------+-------------+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:02:07.342286Z",
     "start_time": "2025-12-13T17:02:07.314145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###dropping rows with missing values in critical columns\n",
    "df=df.na.drop(subset=['dep_time','dep_delay','arr_time','arr_delay',])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:02:59.936747Z",
     "start_time": "2025-12-13T17:02:59.933195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cols_to_exclude = [\n",
    "    \"dep_time\",\n",
    "    \"dep_delay\",\n",
    "    \"taxi_out\",\n",
    "    \"wheels_off\",\n",
    "    \"wheels_on\",\n",
    "    \"taxi_in\",\n",
    "    \"arr_time\",\n",
    "    \"actual_elapsed_time\",\n",
    "    \"air_time\",\n",
    "    \"carrier_delay\",\n",
    "    \"weather_delay\",\n",
    "    \"nas_delay\",\n",
    "    \"security_delay\",\n",
    "    \"late_aircraft_delay\",\n",
    "    \"cancellation_code\",\n",
    "    \"cancelled\",\n",
    "    \"diverted\",\n",
    "    \"fl_date\",\n",
    "    \"op_carrier_fl_num\",\n",
    "    \"origin_city_name\",\n",
    "    \"dest_city_name\"\n",
    "]\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:03:08.705569Z",
     "start_time": "2025-12-13T17:03:08.564645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###drooping unnecessary/post-departure columns as we are trying to predict delay before the flight takes off\n",
    "df=(df\n",
    "    .drop(*cols_to_exclude)\n",
    "    .withColumn(\"label\", (df.arr_delay > 15).cast(\"int\"))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:05:38.556321Z",
     "start_time": "2025-12-13T17:05:38.442238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col,floor\n",
    "df=df.withColumn(\n",
    "    \"crs_dep_hour\",\n",
    "   floor(col(\"crs_dep_time\") / 100).cast(\"int\"))"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:06:07.242456Z",
     "start_time": "2025-12-13T17:06:07.213618Z"
    }
   },
   "cell_type": "code",
   "source": "df=df.drop(\"arr_delay\",\"crs_dep_time\")",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:06:10.001611Z",
     "start_time": "2025-12-13T17:06:09.996013Z"
    }
   },
   "cell_type": "code",
   "source": "df.printSchema()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- op_unique_carrier: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- origin_state_nm: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- dest_state_nm: string (nullable = true)\n",
      " |-- crs_arr_time: integer (nullable = true)\n",
      " |-- crs_elapsed_time: double (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- crs_dep_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:06:33.430787Z",
     "start_time": "2025-12-13T17:06:26.641007Z"
    }
   },
   "cell_type": "code",
   "source": "df.select('label').distinct().show()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T16:26:59.532549Z",
     "start_time": "2025-12-13T16:26:59.525805Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T16:27:01.925462Z",
     "start_time": "2025-12-13T16:27:01.749296Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:06:44.036396Z",
     "start_time": "2025-12-13T17:06:44.032590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cat_cols = [\n",
    "    \"op_unique_carrier\",\n",
    "    \"origin\",\n",
    "    \"dest\",\n",
    "    \"origin_state_nm\",\n",
    "    \"dest_state_nm\"\n",
    "]\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:31:08.207673Z",
     "start_time": "2025-12-13T17:31:08.197140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_cols = [\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day_of_month\",\n",
    "    \"day_of_week\",\n",
    "    \"crs_dep_hour\",\n",
    "    \"crs_elapsed_time\",\n",
    "    \"distance\",\n",
    "    \"carrier_delay_rate\",\n",
    "    \"origin_delay_rate\",\n",
    "    \"dest_delay_rate\",\n",
    "    \"route_delay_rate\"\n",
    "]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:31:29.361128Z",
     "start_time": "2025-12-13T17:31:29.326394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexers=[\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=f\"{c}_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for c in cat_cols\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:31:32.075248Z",
     "start_time": "2025-12-13T17:31:32.059105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols=[f\"{c}_idx\" for c in cat_cols]+num_cols\n",
    "\n",
    "assemble=VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:31:35.321244Z",
     "start_time": "2025-12-13T17:31:35.310105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf=RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=30,\n",
    "    maxDepth=8,\n",
    "    maxBins=347,\n",
    "    minInstancesPerNode=100,\n",
    "    seed=42\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:31:38.248338Z",
     "start_time": "2025-12-13T17:31:38.245252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline(\n",
    "    stages=indexers+[assemble,rf]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:07:43.969050Z",
     "start_time": "2025-12-13T17:07:43.871195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = df.filter(\"month <= 9\")\n",
    "test_df  = df.filter(\"month > 9\")\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:07:55.637638Z",
     "start_time": "2025-12-13T17:07:51.379146Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.count()\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5199988"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "###Adding historical features"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:26:34.638325Z",
     "start_time": "2025-12-13T17:26:34.561589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import avg\n",
    "###Carrier delay rate\n",
    "carries_hist=(\n",
    "    train_df\n",
    "    .groupby(\"op_unique_carrier\")\n",
    "    .agg(avg(\"label\").alias(\"carrier_delay_rate\"))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:26:46.705292Z",
     "start_time": "2025-12-13T17:26:46.687235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###Origin airport delay rate\n",
    "origin_hist=(\n",
    "    train_df\n",
    "    .groupby(\"origin\")\n",
    "    .agg(avg(\"label\").alias(\"origin_delay_rate\"))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:28:10.682301Z",
     "start_time": "2025-12-13T17:28:10.635592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####Destination airport delay rate\n",
    "dest_hist=(\n",
    "    train_df\n",
    "    .groupby(\"dest\")\n",
    "    .agg(avg(\"label\").alias(\"dest_delay_rate\"))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:28:40.540437Z",
     "start_time": "2025-12-13T17:28:40.477390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###route delay rate\n",
    "route_hist = (\n",
    "    train_df\n",
    "    .groupBy(\"origin\", \"dest\")\n",
    "    .agg(avg(\"label\").alias(\"route_delay_rate\"))\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:29:34.935310Z",
     "start_time": "2025-12-13T17:29:34.653091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_hist_features(df):\n",
    "    return (\n",
    "        df\n",
    "        .join(carries_hist, \"op_unique_carrier\", \"left\")\n",
    "        .join(origin_hist, \"origin\", \"left\")\n",
    "        .join(dest_hist, \"dest\", \"left\")\n",
    "        .join(route_hist, [\"origin\", \"dest\"], \"left\")\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "train_df = add_hist_features(train_df)\n",
    "test_df  = add_hist_features(test_df)\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:35:00.162289Z",
     "start_time": "2025-12-13T17:32:11.470313Z"
    }
   },
   "cell_type": "code",
   "source": "rf_model = pipeline.fit(train_df)\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 23:03:57 WARN DAGScheduler: Broadcasting large task binary with size 1184.2 KiB\n",
      "25/12/13 23:04:06 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "[Stage 230:==================================================>     (9 + 1) / 10]\r"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:35:24.136321Z",
     "start_time": "2025-12-13T17:35:08.954100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "predictions.select(\n",
    "    \"label\", \"prediction\", \"probability\"\n",
    ").show(5, truncate=False)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 237:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|0    |0.0       |[0.8453732755886184,0.1546267244113816] |\n",
      "|0    |0.0       |[0.8348296035388334,0.16517039646116657]|\n",
      "|0    |0.0       |[0.8461021290972329,0.15389787090276708]|\n",
      "|0    |0.0       |[0.7835242052669878,0.21647579473301215]|\n",
      "|0    |0.0       |[0.790391494706715,0.20960850529328495] |\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:35:48.137044Z",
     "start_time": "2025-12-13T17:35:29.512878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc=evaluator.evaluate(predictions)\n",
    "print(f\"Test AUC: {auc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.6013\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:16:06.705247Z",
     "start_time": "2025-12-13T17:16:01.962855Z"
    }
   },
   "cell_type": "code",
   "source": "predictions.groupBy(\"label\").count().show()\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1| 277371|\n",
      "|    0|1487908|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:35:58.683283Z",
     "start_time": "2025-12-13T17:35:58.659045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf_stage = rf_model.stages[-1]\n",
    "\n",
    "for name, score in sorted(\n",
    "    zip(feature_cols, rf_stage.featureImportances),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    "):\n",
    "    print(f\"{name:25s} {score:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route_delay_rate          0.3660\n",
      "crs_dep_hour              0.3592\n",
      "month                     0.0998\n",
      "carrier_delay_rate        0.0781\n",
      "origin_delay_rate         0.0396\n",
      "dest_delay_rate           0.0148\n",
      "op_unique_carrier_idx     0.0130\n",
      "day_of_week               0.0087\n",
      "origin_idx                0.0060\n",
      "day_of_month              0.0054\n",
      "origin_state_nm_idx       0.0046\n",
      "dest_idx                  0.0032\n",
      "crs_elapsed_time          0.0011\n",
      "distance                  0.0003\n",
      "dest_state_nm_idx         0.0002\n",
      "year                      0.0000\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:39:49.583830Z",
     "start_time": "2025-12-13T17:39:49.568136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=6,\n",
    "    maxIter=40,\n",
    "    maxBins=347,\n",
    "    seed=42\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:39:51.006927Z",
     "start_time": "2025-12-13T17:39:51.003981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=indexers + [assemble, gbt]\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:04:59.105829Z",
     "start_time": "2025-12-13T17:39:53.089196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = train_df.cache()\n",
    "train_df.count()   # materialize\n",
    "\n",
    "model = pipeline.fit(train_df)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 23:09:53 WARN CacheManager: Asked to cache already cached data.\n",
      "25/12/13 23:15:27 WARN DAGScheduler: Broadcasting large task binary with size 1038.3 KiB\n",
      "25/12/13 23:15:35 WARN DAGScheduler: Broadcasting large task binary with size 1038.8 KiB\n",
      "25/12/13 23:15:40 WARN DAGScheduler: Broadcasting large task binary with size 1043.5 KiB\n",
      "25/12/13 23:15:44 WARN DAGScheduler: Broadcasting large task binary with size 1046.9 KiB\n",
      "25/12/13 23:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1052.2 KiB\n",
      "25/12/13 23:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1069.9 KiB\n",
      "25/12/13 23:16:00 WARN DAGScheduler: Broadcasting large task binary with size 1089.4 KiB\n",
      "25/12/13 23:16:08 WARN DAGScheduler: Broadcasting large task binary with size 1089.9 KiB\n",
      "25/12/13 23:16:12 WARN DAGScheduler: Broadcasting large task binary with size 1094.7 KiB\n",
      "25/12/13 23:16:17 WARN DAGScheduler: Broadcasting large task binary with size 1097.5 KiB\n",
      "25/12/13 23:16:22 WARN DAGScheduler: Broadcasting large task binary with size 1105.9 KiB\n",
      "25/12/13 23:16:29 WARN DAGScheduler: Broadcasting large task binary with size 1126.5 KiB\n",
      "25/12/13 23:16:35 WARN DAGScheduler: Broadcasting large task binary with size 1158.7 KiB\n",
      "25/12/13 23:16:43 WARN DAGScheduler: Broadcasting large task binary with size 1159.2 KiB\n",
      "25/12/13 23:16:49 WARN DAGScheduler: Broadcasting large task binary with size 1163.5 KiB\n",
      "25/12/13 23:16:54 WARN DAGScheduler: Broadcasting large task binary with size 1170.0 KiB\n",
      "25/12/13 23:16:59 WARN DAGScheduler: Broadcasting large task binary with size 1183.0 KiB\n",
      "25/12/13 23:17:05 WARN DAGScheduler: Broadcasting large task binary with size 1202.9 KiB\n",
      "25/12/13 23:17:11 WARN DAGScheduler: Broadcasting large task binary with size 1234.5 KiB\n",
      "25/12/13 23:17:19 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n",
      "25/12/13 23:17:23 WARN DAGScheduler: Broadcasting large task binary with size 1236.2 KiB\n",
      "25/12/13 23:17:30 WARN DAGScheduler: Broadcasting large task binary with size 1246.2 KiB\n",
      "25/12/13 23:17:35 WARN DAGScheduler: Broadcasting large task binary with size 1265.0 KiB\n",
      "25/12/13 23:17:41 WARN DAGScheduler: Broadcasting large task binary with size 1282.5 KiB\n",
      "25/12/13 23:17:48 WARN DAGScheduler: Broadcasting large task binary with size 1310.0 KiB\n",
      "25/12/13 23:17:57 WARN DAGScheduler: Broadcasting large task binary with size 1310.5 KiB\n",
      "25/12/13 23:18:01 WARN DAGScheduler: Broadcasting large task binary with size 1311.5 KiB\n",
      "25/12/13 23:18:07 WARN DAGScheduler: Broadcasting large task binary with size 1316.8 KiB\n",
      "25/12/13 23:18:12 WARN DAGScheduler: Broadcasting large task binary with size 1323.6 KiB\n",
      "25/12/13 23:18:18 WARN DAGScheduler: Broadcasting large task binary with size 1339.1 KiB\n",
      "25/12/13 23:18:26 WARN DAGScheduler: Broadcasting large task binary with size 1359.2 KiB\n",
      "25/12/13 23:18:34 WARN DAGScheduler: Broadcasting large task binary with size 1359.6 KiB\n",
      "25/12/13 23:18:39 WARN DAGScheduler: Broadcasting large task binary with size 1360.6 KiB\n",
      "25/12/13 23:18:45 WARN DAGScheduler: Broadcasting large task binary with size 1366.0 KiB\n",
      "25/12/13 23:18:50 WARN DAGScheduler: Broadcasting large task binary with size 1379.0 KiB\n",
      "25/12/13 23:18:57 WARN DAGScheduler: Broadcasting large task binary with size 1401.0 KiB\n",
      "25/12/13 23:19:04 WARN DAGScheduler: Broadcasting large task binary with size 1429.6 KiB\n",
      "25/12/13 23:19:11 WARN DAGScheduler: Broadcasting large task binary with size 1430.1 KiB\n",
      "25/12/13 23:19:15 WARN DAGScheduler: Broadcasting large task binary with size 1434.0 KiB\n",
      "25/12/13 23:19:22 WARN DAGScheduler: Broadcasting large task binary with size 1434.8 KiB\n",
      "25/12/13 23:19:28 WARN DAGScheduler: Broadcasting large task binary with size 1447.3 KiB\n",
      "25/12/13 23:19:34 WARN DAGScheduler: Broadcasting large task binary with size 1467.5 KiB\n",
      "25/12/13 23:19:42 WARN DAGScheduler: Broadcasting large task binary with size 1491.3 KiB\n",
      "25/12/13 23:19:51 WARN DAGScheduler: Broadcasting large task binary with size 1491.8 KiB\n",
      "25/12/13 23:19:55 WARN DAGScheduler: Broadcasting large task binary with size 1492.8 KiB\n",
      "25/12/13 23:20:02 WARN DAGScheduler: Broadcasting large task binary with size 1498.2 KiB\n",
      "25/12/13 23:20:07 WARN DAGScheduler: Broadcasting large task binary with size 1507.8 KiB\n",
      "25/12/13 23:20:14 WARN DAGScheduler: Broadcasting large task binary with size 1521.0 KiB\n",
      "25/12/13 23:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1560.9 KiB\n",
      "25/12/13 23:20:31 WARN DAGScheduler: Broadcasting large task binary with size 1561.4 KiB\n",
      "25/12/13 23:20:35 WARN DAGScheduler: Broadcasting large task binary with size 1564.5 KiB\n",
      "25/12/13 23:20:42 WARN DAGScheduler: Broadcasting large task binary with size 1565.3 KiB\n",
      "25/12/13 23:20:48 WARN DAGScheduler: Broadcasting large task binary with size 1576.2 KiB\n",
      "25/12/13 23:20:57 WARN DAGScheduler: Broadcasting large task binary with size 1594.4 KiB\n",
      "25/12/13 23:21:04 WARN DAGScheduler: Broadcasting large task binary with size 1619.3 KiB\n",
      "25/12/13 23:21:12 WARN DAGScheduler: Broadcasting large task binary with size 1619.8 KiB\n",
      "25/12/13 23:21:15 WARN DAGScheduler: Broadcasting large task binary with size 1628.1 KiB\n",
      "25/12/13 23:21:21 WARN DAGScheduler: Broadcasting large task binary with size 1629.1 KiB\n",
      "25/12/13 23:21:27 WARN DAGScheduler: Broadcasting large task binary with size 1644.6 KiB\n",
      "25/12/13 23:21:34 WARN DAGScheduler: Broadcasting large task binary with size 1666.0 KiB\n",
      "25/12/13 23:21:40 WARN DAGScheduler: Broadcasting large task binary with size 1691.4 KiB\n",
      "25/12/13 23:21:48 WARN DAGScheduler: Broadcasting large task binary with size 1691.9 KiB\n",
      "25/12/13 23:21:53 WARN DAGScheduler: Broadcasting large task binary with size 1695.0 KiB\n",
      "25/12/13 23:21:58 WARN DAGScheduler: Broadcasting large task binary with size 1695.8 KiB\n",
      "25/12/13 23:22:03 WARN DAGScheduler: Broadcasting large task binary with size 1704.7 KiB\n",
      "25/12/13 23:22:10 WARN DAGScheduler: Broadcasting large task binary with size 1724.3 KiB\n",
      "25/12/13 23:22:16 WARN DAGScheduler: Broadcasting large task binary with size 1749.5 KiB\n",
      "25/12/13 23:22:25 WARN DAGScheduler: Broadcasting large task binary with size 1750.0 KiB\n",
      "25/12/13 23:22:30 WARN DAGScheduler: Broadcasting large task binary with size 1751.1 KiB\n",
      "25/12/13 23:22:36 WARN DAGScheduler: Broadcasting large task binary with size 1754.6 KiB\n",
      "25/12/13 23:22:42 WARN DAGScheduler: Broadcasting large task binary with size 1771.7 KiB\n",
      "25/12/13 23:22:49 WARN DAGScheduler: Broadcasting large task binary with size 1803.7 KiB\n",
      "25/12/13 23:22:57 WARN DAGScheduler: Broadcasting large task binary with size 1853.7 KiB\n",
      "25/12/13 23:23:06 WARN DAGScheduler: Broadcasting large task binary with size 1854.2 KiB\n",
      "25/12/13 23:23:11 WARN DAGScheduler: Broadcasting large task binary with size 1857.3 KiB\n",
      "25/12/13 23:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1858.0 KiB\n",
      "25/12/13 23:23:24 WARN DAGScheduler: Broadcasting large task binary with size 1863.3 KiB\n",
      "25/12/13 23:23:31 WARN DAGScheduler: Broadcasting large task binary with size 1879.4 KiB\n",
      "25/12/13 23:23:37 WARN DAGScheduler: Broadcasting large task binary with size 1907.7 KiB\n",
      "25/12/13 23:23:45 WARN DAGScheduler: Broadcasting large task binary with size 1908.2 KiB\n",
      "25/12/13 23:23:52 WARN DAGScheduler: Broadcasting large task binary with size 1909.4 KiB\n",
      "25/12/13 23:24:01 WARN DAGScheduler: Broadcasting large task binary with size 1919.9 KiB\n",
      "25/12/13 23:24:09 WARN DAGScheduler: Broadcasting large task binary with size 1934.5 KiB\n",
      "25/12/13 23:24:18 WARN DAGScheduler: Broadcasting large task binary with size 1952.9 KiB\n",
      "25/12/13 23:24:27 WARN DAGScheduler: Broadcasting large task binary with size 1971.0 KiB\n",
      "25/12/13 23:24:36 WARN DAGScheduler: Broadcasting large task binary with size 1971.4 KiB\n",
      "25/12/13 23:24:41 WARN DAGScheduler: Broadcasting large task binary with size 1974.4 KiB\n",
      "25/12/13 23:24:49 WARN DAGScheduler: Broadcasting large task binary with size 1975.3 KiB\n",
      "25/12/13 23:24:55 WARN DAGScheduler: Broadcasting large task binary with size 1989.9 KiB\n",
      "25/12/13 23:25:03 WARN DAGScheduler: Broadcasting large task binary with size 2012.9 KiB\n",
      "25/12/13 23:25:11 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/13 23:25:20 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/13 23:25:27 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/13 23:25:34 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/13 23:25:41 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/13 23:25:49 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/13 23:25:58 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:26:08 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:26:16 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:26:24 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:26:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:26:38 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:26:48 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:26:59 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:27:05 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/13 23:27:11 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/13 23:27:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/13 23:27:28 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/12/13 23:27:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:27:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:27:53 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:25 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:35 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:28:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/12/13 23:29:05 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/13 23:29:16 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/13 23:29:26 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/13 23:29:32 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/13 23:29:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/13 23:29:51 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/12/13 23:30:01 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:30:10 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:30:21 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:30:29 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:30:39 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:30:52 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:31:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:31:15 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:31:26 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/12/13 23:31:34 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:31:43 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:31:51 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:32:01 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:32:16 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:32:26 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:32:31 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:32:40 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/13 23:32:48 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:32:59 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:33:08 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:33:19 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:33:24 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:33:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:33:51 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/13 23:34:01 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/13 23:34:13 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/13 23:34:20 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/13 23:34:29 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/13 23:34:38 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/12/13 23:34:48 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T18:08:53.139190Z",
     "start_time": "2025-12-13T18:05:14.127304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds = model.transform(test_df)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(preds)\n",
    "print(\"AUC:\", auc)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 23:36:01 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/12/13 23:36:01 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "[Stage 1211:===========>                                           (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6313798711285877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
